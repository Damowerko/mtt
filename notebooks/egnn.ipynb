{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mtt.data.sparse import SparseData, SparseDataset\n",
    "from mtt.models.egnn import EGNNConv\n",
    "from mtt.models.sparse import SparseInput, SparseLabel, SparseOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 4\n",
    "dataset = SparseDataset(length=input_length, slim=True)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [0.9, 0.1], generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        n_f_in = 1  # time\n",
    "        n_f_out = 2  # sigma, logp\n",
    "        self.n_x_in = 2  # measurement_position, sensor_position\n",
    "\n",
    "        f_hidden = 32\n",
    "        self.x_hidden = 8\n",
    "        n_hidden = 128\n",
    "        ratio = 0.75\n",
    "\n",
    "        self.radius = 100.0\n",
    "\n",
    "        self.f_readin = nn.Linear(n_f_in, f_hidden)\n",
    "        self.egnn = nn.ModuleList(\n",
    "            [\n",
    "                EGNNConv(f_hidden, f_hidden, self.x_hidden, self.x_hidden, n_hidden),\n",
    "                EGNNConv(f_hidden, f_hidden, self.x_hidden, self.x_hidden, n_hidden),\n",
    "                EGNNConv(f_hidden, f_hidden, self.x_hidden, self.x_hidden, n_hidden),\n",
    "                EGNNConv(f_hidden, f_hidden, self.x_hidden, self.x_hidden, n_hidden),\n",
    "            ]\n",
    "        )\n",
    "        self.select = typing.cast(\n",
    "            list[gnn.pool.select.SelectTopK],\n",
    "            nn.ModuleList(\n",
    "                [\n",
    "                    gnn.pool.select.SelectTopK(f_hidden, ratio=ratio),\n",
    "                    gnn.pool.select.SelectTopK(f_hidden, ratio=ratio),\n",
    "                    gnn.pool.select.SelectTopK(f_hidden, ratio=ratio),\n",
    "                    gnn.pool.select.SelectTopK(f_hidden, ratio=ratio),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        self.f_readout = gnn.MLP(\n",
    "            [f_hidden, n_hidden, n_f_out], act=nn.LeakyReLU(), plain_last=True\n",
    "        )\n",
    "\n",
    "    def forward(self, data: SparseData):\n",
    "        positions = data.measurement_position\n",
    "        # h_in.shape = (N, 1)\n",
    "        f_in = data.measurement_time.float()[:, None]\n",
    "        f = self.f_readin.forward(f_in).relu()\n",
    "        # x_in.shape = (N, 2, 2) the last two dimensions are the x,y coordinates\n",
    "        x = torch.stack(\n",
    "            [data.measurement_position, data.sensor_position]\n",
    "            + [torch.zeros_like(data.measurement_position)]\n",
    "            * (self.x_hidden - self.n_x_in),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        batch_idx = torch.repeat_interleave(\n",
    "            torch.arange(\n",
    "                len(data.measurement_batch_sizes),\n",
    "                device=data.measurement_batch_sizes.device,\n",
    "            ),\n",
    "            data.measurement_batch_sizes,\n",
    "        )\n",
    "        for i in range(len(self.egnn)):\n",
    "            # compute graph based on the positions\n",
    "            edge_index = gnn.pool.knn_graph(positions, k=16, batch=batch_idx)\n",
    "            df, dx = self.egnn[i].forward(f, x, edge_index)\n",
    "\n",
    "            # limit the dx update to the radius\n",
    "            dx = self.radius * dx / (torch.norm(dx, dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "            f = f + df\n",
    "            x = x + dx\n",
    "            positions = x[:, 0, :]\n",
    "\n",
    "            # graph pooling\n",
    "            selection = self.select[i].forward(f, batch_idx)\n",
    "            x = x[selection.node_index]\n",
    "            f = f[selection.node_index]\n",
    "            positions = positions[selection.node_index]\n",
    "            batch_idx = batch_idx[selection.node_index]\n",
    "\n",
    "        mu = positions\n",
    "        sigma = nn.functional.softplus(f[:, 0, None]).expand(-1, 2) + 1e-8\n",
    "        logp = nn.functional.logsigmoid(f[:, 1])\n",
    "        return SparseOutput(\n",
    "            mu=mu,\n",
    "            sigma=sigma,\n",
    "            logp=logp,\n",
    "            batch=torch.bincount(batch_idx),\n",
    "        )\n",
    "\n",
    "\n",
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtt.models.sparse import (\n",
    "    logp_loss,\n",
    "    parallel_assignment,\n",
    "    mse_loss,\n",
    "    log_kernel_loss,\n",
    "    kernel_loss,\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, collate_fn=SparseDataset.collate_fn\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-12)\n",
    "model.cuda()\n",
    "\n",
    "log = []\n",
    "for i in range(10):\n",
    "    pbar = tqdm(dataloader)\n",
    "    for data in pbar:\n",
    "        data = SparseData(*(x.cuda() for x in data))\n",
    "        label = SparseLabel.from_sparse_data(data, input_length)\n",
    "\n",
    "        output = model.forward(data)\n",
    "\n",
    "        x_split_idx = output.batch.cumsum(0)[:-1].cpu()\n",
    "        y_split_idx = label.batch.cumsum(0)[:-1].cpu()\n",
    "        mu_split = output.mu.tensor_split(x_split_idx)\n",
    "        sigma_split = output.sigma.tensor_split(x_split_idx)\n",
    "\n",
    "        logp_split = output.logp.tensor_split(x_split_idx)\n",
    "        y_split = label.y.tensor_split(y_split_idx)\n",
    "\n",
    "        batch_size = output.batch.shape[0]\n",
    "        loss = torch.zeros((batch_size,), device=output.mu.device)\n",
    "        for batch_idx in range(batch_size):\n",
    "            if mu_split[batch_idx].shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            loss[batch_idx] += kernel_loss(\n",
    "                mu_split[batch_idx],\n",
    "                logp_split[batch_idx].exp(),\n",
    "                y_split[batch_idx],\n",
    "                50.0,\n",
    "            )\n",
    "\n",
    "        # for batch_idx, i, j in parallel_assignment(mu_split, y_split, logp_split):\n",
    "        #     loss[batch_idx] += logp_loss(\n",
    "        #         mu_split[batch_idx],\n",
    "        #         sigma_split[batch_idx],\n",
    "        #         logp_split[batch_idx],\n",
    "        #         y_split[batch_idx],\n",
    "        #         (i, j),\n",
    "        #     )\n",
    "        # loss[batch_idx] += mse_loss(\n",
    "        #     mu_split[batch_idx],\n",
    "        #     y_split[batch_idx],\n",
    "        #     (i, j),\n",
    "        # )\n",
    "        loss = loss.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log.append(\n",
    "            {\n",
    "                \"loss\": loss.item(),\n",
    "                \"sigma\": output.sigma.mean().item(),\n",
    "            }\n",
    "        )\n",
    "        pbar.set_postfix(log[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].plot([x[\"loss\"] for x in log])\n",
    "axs[1].plot([x[\"sigma\"] for x in log])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# plot the mu positions for a sample in the dataset\n",
    "sample = dataset.get(np.random.randint(10000), 50)\n",
    "with torch.no_grad():\n",
    "    label = SparseLabel.from_sparse_data(sample, input_length)\n",
    "    output = model.cpu().forward(sample)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    axs[0].scatter(*sample.measurement_position.T, c=\"blue\")\n",
    "    axs[0].scatter(*output.mu.cpu().T, c=\"black\")\n",
    "    axs[0].scatter(*label.y.cpu().T, c=\"red\")\n",
    "\n",
    "    # make an image using the output.logp.exp() as the intensity of a gaussian kernel with sigma = 10.0\n",
    "    XY = torch.cartesian_prod(*[torch.linspace(-500, 500, 128)] * 2)\n",
    "    dist = (XY[:, None, :] - output.mu[None, ...]).norm(dim=-1)\n",
    "    # K = torch.exp(-(dist**2) / (2 * output.sigma.mean(-1)[None, :] ** 2))\n",
    "    K = torch.exp(-(dist**2) / (2 * 20**2))\n",
    "    Z = (K @ output.logp.exp().squeeze()).reshape(128, 128).cpu().numpy()\n",
    "\n",
    "axs[1].imshow(Z.T, extent=(-500, 500, -500, 500), origin=\"lower\", cmap=\"viridis\")\n",
    "axs[1].scatter(*label.y.cpu().T, c=\"red\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-500, 500)\n",
    "    ax.set_ylim(-500, 500)\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.measurement_position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=32, collate_fn=SparseDataset.collate_fn\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for data in val_dataloader:\n",
    "    data = typing.cast(SparseData, data)\n",
    "    input = SparseInput.from_sparse_data(data, input_length)\n",
    "    label = SparseLabel.from_sparse_data(data, input_length)\n",
    "\n",
    "    estimates, clutter_prob = model.forward(\n",
    "        Mixture(input.x_pos, input.x, input.x_batch)\n",
    "    )\n",
    "    is_clutter = data.is_clutter.float()\n",
    "\n",
    "    tp += ((clutter_prob > 0.5) & (is_clutter == 1)).sum()\n",
    "    tn += ((clutter_prob <= 0.5) & (is_clutter == 0)).sum()\n",
    "    fp += ((clutter_prob > 0.5) & (is_clutter == 0)).sum()\n",
    "    fn += ((clutter_prob <= 0.5) & (is_clutter == 1)).sum()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(\n",
    "    f\"Accuracy: {accuracy:.2f}, F1: {f1:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
