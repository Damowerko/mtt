{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from mtt.data.sparse import SparseData, SparseDataset\n",
    "from mtt.models.kernel import KernelEncoderLayer, KernelDecoderLayer, Mixture\n",
    "from mtt.models.sparse import SparseInput, SparseLabel, SparseOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 4\n",
    "dataset = SparseDataset(length=input_length, slim=True)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [0.9, 0.1], generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.readin = gnn.MLP([2 * input_length, 256, 32], act=nn.LeakyReLU())\n",
    "        self.encoder_layers = typing.cast(\n",
    "            list[KernelEncoderLayer],\n",
    "            nn.ModuleList(\n",
    "                [\n",
    "                    KernelEncoderLayer(2, 25, 32, 256, 10.0, False),\n",
    "                    KernelEncoderLayer(2, 25, 32, 256, 50.0, False),\n",
    "                    KernelEncoderLayer(2, 25, 32, 256, 25.0, False),\n",
    "                    KernelEncoderLayer(2, 25, 32, 256, 10.0, False),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        self.clutter_readout = gnn.MLP(\n",
    "            [32, 256, 1], plain_last=True, act=nn.LeakyReLU()\n",
    "        )\n",
    "        self.decoder_layers = typing.cast(\n",
    "            list[KernelDecoderLayer],\n",
    "            nn.ModuleList(\n",
    "                [\n",
    "                    KernelDecoderLayer(2, 25, 32, 256, 25.0, 100.0, False),\n",
    "                    KernelDecoderLayer(2, 25, 32, 256, 25.0, 50.0, False),\n",
    "                    KernelDecoderLayer(2, 25, 32, 256, 25.0, 25.0, False),\n",
    "                    KernelDecoderLayer(2, 25, 32, 256, 25.0, 10.0, False),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        self.readout = gnn.MLP([32, 256, 4], plain_last=True, act=nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, x: Mixture) -> tuple[SparseOutput, torch.Tensor]:\n",
    "        # encoder\n",
    "        e = x.map_weights(self.readin.forward)\n",
    "        for layer in self.encoder_layers:\n",
    "            e = layer.forward(e)\n",
    "\n",
    "        clutter_prob = self.clutter_readout(e.weights).softmax(dim=-1)\n",
    "\n",
    "        # decoder\n",
    "        batch_size = 1 if x.batch is None else x.batch.shape[0]\n",
    "        with torch.no_grad(), torch.device(x.weights.device):\n",
    "            d_pos = (\n",
    "                torch.cartesian_prod(*[torch.linspace(-500, 500, 5)] * 2)\n",
    "                .reshape(-1, 2)\n",
    "                .repeat_interleave(batch_size, dim=0)\n",
    "            )\n",
    "\n",
    "            d_weights = torch.zeros(\n",
    "                d_pos.shape[0],\n",
    "                e.weights.shape[1],\n",
    "            )\n",
    "            d_batch = torch.full((batch_size,), d_pos.shape[0] // batch_size)\n",
    "            z = Mixture(d_pos, d_weights, d_batch)\n",
    "        for layer in self.decoder_layers:\n",
    "            z = layer.forward(z, e)\n",
    "\n",
    "        # readout\n",
    "        z = z.map_weights(self.readout.forward)\n",
    "        delta_mu, sigma, logits = torch.split(z.weights, [2, 1, 1], dim=-1)\n",
    "        mu = z.positions + delta_mu\n",
    "        sigma = nn.functional.softplus(sigma).expand(-1, 2) + 1e-8\n",
    "        logits = nn.functional.logsigmoid(logits)[..., 0]\n",
    "        assert z.batch is not None\n",
    "        estimates = SparseOutput(mu, sigma, logits, z.batch)\n",
    "        return estimates, clutter_prob\n",
    "\n",
    "\n",
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtt.models.sparse import logp_loss, parallel_assignment\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, collate_fn=SparseDataset.collate_fn\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-8)\n",
    "\n",
    "log = []\n",
    "for i in range(10):\n",
    "    pbar = tqdm(dataloader)\n",
    "    for data in pbar:\n",
    "        data = SparseData(*(x.cuda() for x in data))\n",
    "        input = SparseInput.from_sparse_data(data, input_length)\n",
    "        label = SparseLabel.from_sparse_data(data, input_length)\n",
    "\n",
    "        output, clutter_prob = model.forward(\n",
    "            Mixture(input.x_pos, input.x, input.x_batch)\n",
    "        )\n",
    "\n",
    "        clutter_loss = nn.functional.binary_cross_entropy(\n",
    "            clutter_prob.squeeze(), data.is_clutter.float()\n",
    "        )\n",
    "\n",
    "        x_split_idx = output.batch.cumsum(0)[:-1].cpu()\n",
    "        y_split_idx = label.batch.cumsum(0)[:-1].cpu()\n",
    "        mu_split = output.mu.tensor_split(x_split_idx)\n",
    "        sigma_split = output.sigma.tensor_split(x_split_idx)\n",
    "\n",
    "        logp_split = output.logp.tensor_split(x_split_idx)\n",
    "        y_split = label.y.tensor_split(y_split_idx)\n",
    "\n",
    "        batch_size = output.batch.shape[0]\n",
    "        logp = torch.zeros((batch_size,), device=output.mu.device)\n",
    "        for batch_idx, i, j in parallel_assignment(mu_split, y_split, None):\n",
    "            logp[batch_idx] = logp_loss(\n",
    "                mu_split[batch_idx],\n",
    "                sigma_split[batch_idx],\n",
    "                logp_split[batch_idx],\n",
    "                y_split[batch_idx],\n",
    "                (i, j),\n",
    "            )\n",
    "        logp = logp.mean()\n",
    "        loss = logp + clutter_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log.append(\n",
    "            {\n",
    "                \"logp\": logp.item(),\n",
    "                \"clutter_loss\": clutter_loss.item(),\n",
    "                \"loss\": loss.item(),\n",
    "                \"sigma\": output.sigma.mean().item(),\n",
    "            }\n",
    "        )\n",
    "        pbar.set_postfix(log[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].plot([x[\"logp\"] for x in log])\n",
    "axs[1].plot([x[\"clutter_loss\"] for x in log])\n",
    "axs[2].plot([x[\"sigma\"] for x in log])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mu positions for a sample in the dataset\n",
    "sample = dataset.get(100, 50)\n",
    "with torch.no_grad():\n",
    "    input = SparseInput.from_sparse_data(sample, input_length)\n",
    "    label = SparseLabel.from_sparse_data(sample, input_length)\n",
    "    output, _ = model.forward(Mixture(input.x_pos, input.x, input.x_batch))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    axs[0].scatter(*output.mu.cpu().T, c=\"black\")\n",
    "    axs[0].scatter(*label.y.cpu().T, c=\"red\")\n",
    "\n",
    "    # make an image using the output.logp.exp() as the intensity of a gaussian kernel with sigma = 10.0\n",
    "    XY = torch.cartesian_prod(*[torch.linspace(-500, 500, 128)] * 2)\n",
    "    dist = (XY[:, None, :] - output.mu[None, ...]).norm(dim=-1)\n",
    "    K = torch.exp(-(dist**2) / (2 * output.sigma.mean(-1)[None, :] ** 2))\n",
    "    Z = (K @ output.logp.exp().squeeze()).reshape(128, 128).cpu().numpy()\n",
    "\n",
    "axs[1].imshow(Z.T, extent=(-500, 500, -500, 500), origin=\"lower\", cmap=\"viridis\")\n",
    "axs[1].scatter(*label.y.cpu().T, c=\"red\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-500, 500)\n",
    "    ax.set_ylim(-500, 500)\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=32, collate_fn=SparseDataset.collate_fn\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for data in val_dataloader:\n",
    "    data = typing.cast(SparseData, data)\n",
    "    input = SparseInput.from_sparse_data(data, input_length)\n",
    "    label = SparseLabel.from_sparse_data(data, input_length)\n",
    "\n",
    "    estimates, clutter_prob = model.forward(\n",
    "        Mixture(input.x_pos, input.x, input.x_batch)\n",
    "    )\n",
    "    is_clutter = data.is_clutter.float()\n",
    "\n",
    "    tp += ((clutter_prob > 0.5) & (is_clutter == 1)).sum()\n",
    "    tn += ((clutter_prob <= 0.5) & (is_clutter == 0)).sum()\n",
    "    fp += ((clutter_prob > 0.5) & (is_clutter == 0)).sum()\n",
    "    fn += ((clutter_prob <= 0.5) & (is_clutter == 1)).sum()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(\n",
    "    f\"Accuracy: {accuracy:.2f}, F1: {f1:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
